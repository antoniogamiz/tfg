\chapter{Introducción}

Un tropo, como describe Rizzo Michael \cite{rizzo2013art}, es una imagen
universalmente reconocida, con varias capas de significado contextual que
crean un nueva metáfora visual. Un buen ejemplo, es el tropo \emph{Puedo explicarlo} \cite{tropo:ICanExplain}, donde un caracter que se siente culpable,
reacciona a la aparición de una figura autoritaria diciendo: \emph{"¡Puedo
    explicarlo!"}, procediendo a explicar lo que sea que haya pasado. Este tropo ha
sido enormemente utilizado en una infinidad de situaciones en obras culturales
\cite{tropo:ICanExplain}, desde \emph{Iron Man} hasta la serie de animación
\emph{Digimon}.

Cada película, libro o teatro, está formado por una inmensa cantidad de tropos
y, en cierta medida, su trama es caracterizada por ellos. Esa misma
caracterización, motiva un gran interés en el estudio de estos tropos: cómo se
relacionan entre ellos, cómo de comunes son, qué implicaciones tiene que una
obra cultural presente un cierto conjunto de tropos, etc.

Ese estudio es la principal motivación de este trabajo. Dado un conjunto de
obras culturales y los tropos que aparecen en ellas, se quiere estudiar la
relación existente entre los tropos y su rating, para obtener el máximo \emph
{rating} posible. En particular, este trabajo se centra en el estudio de tropos
encontrados en películas.

El trabajo está organizado en diferentes bloques:

\begin{enumerate}
    \item Bloque I: se describe en detalle el problema que se trata en el
          trabajo, seguido del estado del arte en el uso del procesamiento del lenguaje natural(o NLP, del inglés \textit{Natural Language Processing}) para encontrar
          relaciones entre palabras. A continuación, se detalla la planificación
          seguida para resolver el problema, así como las prácticas seguidas para
          realizarla.
    \item Bloque II: desarrollo teórico de la base matemática usada para la
          realización de este trabajo.
    \item Bloque III: se describe el proceso de análisis e implementación
          llevados a cabo, así como unas conclusiones finales.
\end{enumerate}

Este proyecto es software libre, y está liberado con la licencia \cite{gplv3}.

\section*{Definiciones previas}

Antes de pasar a la descripción del problema, es necesario definir algunos conceptos importantes para
este trabajo.

\begin{definition}[Dato]
    Se define como dato a la unidad de información más pequeña con la que se trabajar.
\end{definition}

\begin{definition}[Conjunto de datos]
    Se define como conjunto de datos a una agrupación de datos.
\end{definition}

En la mayor parte del desarrollo matemático, se entenderá como datos a las palabras del vocabulario que se use. Como conjunto de datos
se usará el corpus de texto (que está formado por palabras). Al final del desarrollo, los datos pasarán a ser tropos, que es el objetivo principal
de este trabajo. Sin embargo, como se observa en las conclusiones finales, este desarrollo también es válido para otros tipos de datos, como ingredientes
de recetas \cite{kazama2018neural}.

\subsection*{\textit{Embeddings}}

En el análisis de datos de texto se suele encontrar el problema de representar la semántica de un conjunto de datos.
En general, lo que se quiere es extraer el significado de esos datos. Esto permite entender, interpretar los datos
a partir de su estructura (por ejemplo, a partir de frases y párrafos, si los datos son texto) e identificar relaciones
entre ellos.

A raíz de eso, surgió el Análisis Semántico Latente (o más conocido por LSA, \textit{Latent Semantic Analysis} \cite{dumais2004latent}),
que es una técnica popular para analizar relaciones entre las palabras de un texto. Básicamente transforma datos en forma de texto en vectores de $d\in\mathbb{N}$
componentes o características latentes (es decir, escondidas), donde $d<m$, siendo $m\in\mathbb{N}$, el número de
datos. Una de las primeras menciones del término \textit{embedding} puede ser encontrada en \cite{landauer1997learning}.
En este trabajo, un \textit{embedding} se va a definir como:

\begin{definition}[\textit{Embedding}]
    Dado un conjunto de datos $D$, de tamaño $m\in\mathbb{N}$, un elemento $w\in D$, el embedding de $w$ es
    la representación en un vector de $d$ dimensiones (o características) con $d<m$.
\end{definition}

Este concepto puede ser usado para codificar la semántica de un término, en un espacio vectorial de dimensión reducida
y fija, $m \in \mathbb{N}$, de forma que se pueda operar con esas representaciones de forma sencilla y eficiente. A continuación se ve cómo
se deduce la semántica del término a partir de su \textit{embedding}. Normalmente si el conjunto de datos son palabras, el nombre
que se suele usar es \textit{word embedding}. Es importante notar que $D$ normalmente es un grupo de palabras, pero también puede ser,
por ejemplo, un grupo de frases \cite{lin2017structured} o un grupo de tropos, como es el caso de este trabajo.

\subsection*{Semántica de un embedding}

La motivación para usar \textit{embeddings} con palabras proviene de la idea de John Rupert
Firth de que `una palabra se caracteriza por la compañía que mantiene' \cite{firth1957synopsis}. Por ejemplo, si no se sabe
el significado de la palabra \textit{coche}, pero aparece en las siguientes frases (o contextos):
\begin{itemize}
    \item El coche es muy importante para el transporte en las zonas rurales.
    \item En la autovía, la velocidad máxima de un coche son 120 kilómetros por hora.
    \item El coche y otros vehículos...
\end{itemize}
Y suponiendo que se han visto esas mismas palabras en otros contextos:
\begin{itemize}
    \item Un camión es usado para el transporte de grandes mercancías.
    \item Un camión no puedo alcanzar gran velocidad.
\end{itemize}
Del hecho de que \textit{coche} aparezca al lado de palabras como \textit{transporte}, \textit{velocidad} y \textit{vehículo},
de igual forma aparecen para camión, sugiere que un coche y un camión son similares (ambos son vehículos).

También hay ejemplos con tropos. Si se suma \href{https://tvtropes.org/pmwiki/pmwiki.php/Main/AmicableExes}{\textit{Exs amigables}}
con \href{https://tvtropes.org/pmwiki/pmwiki.php/Main/AccidentNotMurder}{\textit{Accidente, no asesinato}}, entonces una solución con sentido sería
\href{https://tvtropes.org/pmwiki/pmwiki.php/Main/MyGodWhatHaveIDone}{\textit{Oh Dios mío, qué he hecho}}.

Usando esta intuición, ahora se va a introducir como detectar estas relaciones matemáticamente.

Dada una palabra, se puede generar un vector que represente el valor semántico de tal palabra (\textit{embedding}), es decir, qué significa la palabra, qué caracteriza
su significado, etc. Esta representación vectorial se genera de forma que se puedan obtener otras palabras
cercanas `semánticamente' (dada la representación vectorial de la palabra \textit{coche}, la representación de la palabra \textit{camión}
debería ser cercana semánticamente a la de camión porque ambas son vehículos). Para medir este concepto se va a definir una forma de medir el nivel
de cercanía semántica entre dos palabras. Para ello, se suelen usar funciones de medida (o lo más parecido a ellas):

\begin{definition}
    Sea $d: A\times A \longrightarrow \mathbb{R}$ una función, donde $A$ es un conjunto cualquiera de elementos. Se dice que $d$ es una métrica o función
    de medida si cumple las siguientes propiedades:
    \begin{enumerate}
        \item No negatividad: sean $x,y\in A$, entonces $d(x,y)\geq 0$. $d(x,y)=0$ si y solo si $x=y$.
        \item Simetría: sean $x,y\in A$, entonces $d(x,y) = d(y,x)$.
        \item Desigualdad triangular o de Schwarz: sean $x,y,z\in A$, entonces $d(x,y) \leq d(x,z) + d(z,y)$.
    \end{enumerate}
\end{definition}

Una posible elección sería usar la similitud coseno:

\begin{definition}[Similitud coseno]\label{def:similitud_coseno}
  Dados dos vectores $a,b\in\mathbb{R}$, $n\in\mathbb{N}$, se define la similitud coseno como:
  \[
    S_c(a,b)=\cos\theta = \frac{a\cdot b}{\norm{a}\norm{b}} \in [-1, 1]
  \]
donde $\theta\in[-\pi,\pi]$ es el ángulo entre los vectores $a$ y $b$.
\end{definition}

Aunque es importante notar que esta función no es de medida, ya que no cumple dos de las propiedades:
\begin{proof}
    $ $
    \begin{itemize}
    \item Si $A=\mathbb{R}^2$, entonces se tiene que $S_c\left( (1,1), (2,2) \right) = 0$ pero $(1,1)\neq (2,2)$.
    \item Si $A=\mathbb{R}^2$, entonces tomando $x=(1,0)$, $y=(0,1)$ y $z=(1,1)$, se tiene que $S_c\left( x, y \right) > S_c(x,z) + S_c(z,y)$.
    \end{itemize}
\end{proof}

Aunque esta función no sea una métrica, se usa para medir la cercanía semántica porque la magnitud de los vectores usados para las representaciones no aporta
ningún valor semántico \cite{landauer1997learning}, sino que es la dirección del vector la que lo aporta. Sea un embedding de dimensión $d\in\mathbb{N}$, es decir,
se tienen $d$ componentes numéricas por cada embedding. El valor de las componentes (o más bien las combinaciones lineales) de esos embeddings,
codifican el significado semántico de cada palabra. Un famoso ejemplo de este cálculo sería \emph{rey} - \emph{hombre} + \emph{mujer} = \emph{reina} \cite{drozd-etal-2016-word}.
De ahí se puede deducir que hay una cierta dirección que más o menos apunta al concepto de realeza y otra dirección que apunta al concepto de género.

A partir de ese ejemplo, se puede ver perfectamente la razón de que la magnitud de los vectores no sea relevante pero sí lo sea su dirección. Sean $a,b\in\mathbb{R}^d$ dos embeddings
distintos, de forma que $a=(-1, 2, 3)$ y $b=(-3, 6, -9)$. Estos dos vectores tienen magnitudes totalmente distintas ($a=3b$), pero apuntan hacia la misma dirección
(por lo que tienen similitud coseno \ref{def:similitud_coseno} igual a 0, $S_c(a,b)=0$). Eso es el resultado esperable porque significa que tienen la misma proporción relativa en cada componente.
Sin embargo, si se hubiera usado la distancia euclídea usual, la distancia entra $a$ y $b$ sería de aproximadamente $7.48$. Sería fácil encontrar otro vector $c$ igual de cercano (distancia euclídea) a $a$
que $b$, pero probablemente apuntaría en una dirección totalmente distinta.

Con estos conceptos definidos, se puede definir el problema y ver el estado del arte en cuanto a la generación y uso de estos embeddings.