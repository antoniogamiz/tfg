\chapter{Implementación}

La implementación del software se ha dividido en hitos. Estos, han sido definidos en Github
y cada uno de ellos contiene un grupo de \textit{issues} que se corresponden con las distintas
mejoras que se han ido incorporando al software a lo largo de su desarrollo.

\section{Configuración inicial}

En este primer hito lo que se ha hecho es preparar las herramientas necesarias para poder desarrollar
el proyecto de forma cómoda. Esto incluye tareas como:

\begin{itemize}
  \item Detección de faltas: cada vez que se hace cualquier cambio en el texto del proyecto, se lanza un proceso
  automático que comprueba que cada palabra esté en un determinado diccionario (español en este caso).
  \item Definición de los objetivos y las historias de usuario: para poder empezar a desarrollar el trabajo es importante
  tener claro qué se va a hacer en él y en qué partes se va a dividir ese trabajo. Como se comentaba en la metodología, se
  ha seguido una metodología \textit{agile} en la que se intenta que cada nueva adición al trabajo aporte valor al usuario final
  (que en este caso es, principalmente, el tribunal).
  \item Compilación del proyecto: por razones evidentes se ha usado \textit{LaTex} para escribir el trabajo. El proceso de compilación
  requiere de configuración extra por lo que este proceso se ha automatizado de forma que:
  \begin{itemize}
    \item Cualquier persona que se descargue el código fuente pueda compilar el proyecto de forma cómoda.
    \item Cada vez que se integre una nueva parte al proyecto, éste se construya y publique automáticamente para facilidad de los tutores
    del trabajo que no puedan compilar el proyecto en su ordenador.
  \end{itemize}
\end{itemize}

Las herramientas que se han usado para implementar el trabajo son:

\begin{enumerate}
  \item Lenguaje de programación: se ha escogido \textit{Python} porque es un lenguaje muy famoso en el ámbito de la inteligencia artificial y el análisis de datos. Además
  tiene un coste de aprendizaje muy bajo, por lo que el código puede ser entendido fácilmente en poco tiempo (a diferencia de otras alternativas como \textit{C++}). \textit{Javascript}
  también es un lenguaje muy conocido, pero presenta muchas peculiaridades que hacen que sea difícil de entender en ocasiones. Por eso y por el hecho de que este trabajo no contiene ninguna
  interfaz web, también se ha descartado esa opción.
  \item Gestor de tareas: se ha escogido \textit{poetry} porque agrupa muchas de las funcionalidades necesarias para este trabajo. Como levantar entornos virtuales,
  gestión de dependencias y soporte para scripts. Hay otras alternativas como \textit{pip} (solo incluye instalación de paquetes) o \textit{conda} (que tiene flujos de trabajo poco intuitivos).
  \item Conjunto de datos: se ha optado por escoger los datos de \href{https://github.com/rhgarcia/tropescraper}{TropeScraper}, porque además de incluir conjuntos de datos listos para descargar, provee
  un paquete en Python para generar un conjunto de datos actualizados en cualquier momento.
\end{enumerate}

\section{Estructuras de datos}

Uno de los objetivos de la implementación es permitir a otras persones (como científicos de datos) poder usar este modelo y modificarlo sin restricciones. Para ello, se han definido varias
estructuras de datos que permiten este objetivo. Se han usado términos anglosajones el código debido a que es el lenguaje por defecto en la comunidad.

\begin{itemize}
  \item \textit{Data}: es la únidad básica de la red neuronal. El único requerimiento que tienen los inputs de la red es que se pueda generar un hash a partir de ellos. Para enforzar esta regla
  a nivel de código se ha usado una potente característica de python conocida como \textit{tipado sustructural}, que presenta varios beneficios \cite{walker2005substructural}. El principal siendo
  que el cliente del módulo no tiene por qué heredar explícitamete de clases provistes por el módulo, creando un código menos acoplado.
  \item \textit{Vocabulary}: esta clase se usa para representar un conjunto de \textit{Data}. Dado que el algoritmo es \textit{any2vec}, el vocabulario puede estar formado por cualquier tipo de
  estructuras.
\end{itemize}

\subsection{Vectores codificados \textit{one-hot}}

Las estructuras \textit{Data} y \textit{Vocabulary} se pueden usar ahora para generar vectores \textit{one-hot}. Se recuerda que los vectores \textit{one-hot} tienen un tamaño $V$, siendo $V$ el tamaño del vocabulario, y
una de sus entradas vale 1. Esto puede suponer un problema de eficiencia para vocabularios muy grandes, como es el caso de los tropos. Para solucionar este problema se ha optado por no representar
estas estructuras internamente como vectores, sino simplemente almacenar los índices que tienen un 1 almacenado. Esto hace que las operaaciones matriciales se conviertan en simplemente copiar filas o columnas a otra estructura, reduciendo el tiempo de ejecución considerablemente.

Aunque los vectores $one-hot$ solo puedan, en principio, tener una de sus entradas a 1, por razones de eficiencia en este modelo se ha optado por eliminar esta restricción. Lo que permite acelerar
las operaciones con el contexto (en Skip-Gram) y las opereaciones con el input (en el caso de Bolsa Continua de Palabras).

\subsection{Any2Vec}

Esta clase es la encargada de coordinar los pasos de propagación hacia delante y atrás, así cómo de registrar el error acumulado. El hecho de haber usado una aproximación dirigida a objetos
seguramente tenga un efecto en la eficiencia del programa, pero añade bastante claridad al módulo y a su uso, así que se ha estimado beneficioso.

Como se puede observar, esta clase usa generadores para producir los pares de entrenamiento $(target, context)$. Esto se ha decidido así para aprovechar al máximo la memoria de la máquina donde
se corra el programa. La alternativa sería leer el conjunto de datos en su totalidad (lo que en el caso de los tropos no supone un gran problema). Posteriormente, habría que generar todos los pares.
Esto puede suponer un gran consumo de memoria, dependiendo en la estrategia elegida para generar los pares. Otra consideración importante es que el uso de generadores está \textit{envuelto} por una
función que, valga la redundancia, los genera. Este tipo de funciones es conocida como \textit{closure}. Esto es necesario ya que los generadores solo pueden ser recorridos una vez.

Any2Vec también acepta matrices de pesos ya inicializadas, así se permite al consumidor ejecutar el modelo con pesos de antiguas iteraciones o generados usando otras técnicas. También se permite
al consumidor guardar el progreso de entrenamiento. De esto se encarga la interfaz \textit{ModelStateInterface}. Se ha usado inyección de dependencias (parte de los principios \textit{SOLID} \cite{martin2000design})
para permitir al consumidor usar el mejor proveedor que él estime oportuno. En el caso de este trabajo, se ha optado por simplemente usar un almacenamiento básico en formato CSV, directamente en el sistema de
archivos de la máquina que ejecuta el modelo.

