\chapter{Redes neuronales}

Este trabajo gira en torno al algoritmo \textit{Word2Vec} \cite{word2vec:1} \cite{word2vec:2}, una técnica de procesamiento
de lenguaje natural, que usa \textit{aprendizaje profundo} para su funcionamiento. El aprendizaje
profundo es una rama del Aprendizaje Máquina, que a su vez está incluido dentro del campo más general
de la Inteligencia Artificial. En general, el objetivo del aprendizaje máquina es estudiar algoritmos
que mejoran a partir de la experiencia. Tom M. Mitchell \cite{mitchell1997} enunció una definición formal
del término:

\begin{definition}[Aprender]
    Se dice que un programa \textit{aprende} de una experiencia \textit{E}, con respecto a un grupo
    de tareas \textit{T} y una medida de rendimiento \textit{P}, si su rendimiento en las tareas de
    \textit{T}, medido por \textit{P}, mejora con la experiencia \textit{E}.
\end{definition}

Estas técnicas pueden estar basadas en el concepto biológico de redes neuronales, las cuales están formadas por neuronas.
A lo largo de este capítulo, se va a formalizar este concepto de neurona y se va a definir cómo interactúan entre ellas.

\section{Introducción}

Las redes neuronales suelen ser usadas en objetos que, a priori, no tienen una representación matemática,
como palabras o tropos. El procedimiento habitual es encontrar una representación numérica que permita, de forma
cómoda, operar con esos objetos. Por ejemplo, para representar una imagen en blanco y negro, se puede
tomar un vector $x$, con tantas entradas como píxeles, donde cada entrada es un valor entre 0 y 1,
representando el valor del color en ese píxel. La red neuronal ahora debe tomar ese valor de entrada,
y producir otra representación matemática distinta y compacta, que también debe ser interpretada. Por ejemplo,
si se quieren detectar números escritos en la imagen, la representación de salida puede ser un vector
de 10 entradas, donde el valor de cada entrada será igual a 1 si el índice de esa entrada coincide con
el número presente en la imagen.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            roundnode/.style={circle, draw=black, minimum size=7mm},
        ]

        \node[roundnode]   at (1,4)   (input2)  {$x_2$};
        \node[roundnode]   at (1,6)   (input1)  {$x_1$};
        \node[roundnode]   at (5,4)   (hidden2)  {};
        \node[roundnode]   at (5,6)   (hidden1)  {};
        \node[roundnode]   at (7,5)   (output)  {};

        \draw[-stealth] (input1) -- node[above,xshift=1cm]   {$w_1$} (hidden1);
        \draw[-stealth] (input2) -- node[above,xshift=1cm, yshift=-0.1cm]   {$w_2$} (hidden1);
        \draw[-stealth] (input1) -- node[above, xshift=1cm, yshift=-0.7cm] {$w_3$} (hidden2);
        \draw[-stealth] (input2) -- node[below, xshift=1cm]   {$w_4$} (hidden2);

        \draw[-stealth] (hidden1) -- node[above]   {$w_5$} (output);
        \draw[-stealth] (hidden2) -- node[below]   {$w_6$} (output);

        \draw[] (output) -- node[right, xshift=0.3cm] {$y$} (8,5);

    \end{tikzpicture}
    \caption{Ejemplo de red neuronal con una capa escondida}
    \label{redneuronal:1}
\end{figure}

\subsection{Conceptos iniciales}

Para a partir de un vector de entrada $x\in\mathbb{R}^n$, producir un vector de salida $v\in\mathbb{R}^m$,
con el resultado que se quiere, será necesario realizar ciertas operaciones entra cada una de las capas
de la red neuronal, como se muestra en la Figura \ref{redneuronal:1}. Cada una de estas operaciones, activará
algunas neuronas en cada capa, produciendo al final el vector deseado. Una neurona no es más ni menos
que un escalar, entre 0 y 1 normalmente, cuyo valor es calculado usando el valor de las neuronas de la
capa anterior (o del input, si es la segunda capa) junto con el valor de los pesos $w_i$:

\[
    a^{(n+1)} = a^{(n)}_1w^{(n)}_1+a^{(n)}_2w^{(n)}_2+\cdots+a^{(n)}_nw^{(n)}_n=\sum_{i=1}^na^{(n)}_iw^{(n)}_i\in\mathbb{R}
\]

Los pesos no son más que números arbitrarios, que son actualizados en cada fase de aprendizaje \cite{rumelhart1986learning}.
Es trivial comprobar que $a^{(n+1)}$ es un número real, de cualquier valor. Esto es problemático porque
lo que se quiere ahora es saber cuando una neurona está \textit{activa}. Para ello, se puede usar lo
que se denomina una \textit{función de activación}, que no es ni más ni menos que una función  que transforma
la recta real en el intervalo $[0,1]$. Intuitivamente, el valor que devuelve la función de activación dice
cómo de positivo es el valor de esa neurona. Ahora ya se puede definir formalmente nuestra neurona:

\begin{definition}[Neurona]
    Sea $w\in\mathbb{R}^n$ y $\theta:\mathbb{R}^n \longrightarrow \mathbb{R}$ una función de activación. Una neurona calcula la función
    $h: \mathbb{R}^n \times \mathbb{R}^n \longrightarrow \mathbb{R}$ dada por:
    \[
        h(w, a) = \theta\left( \sum_{i=1}^d w_ia_i \right), \;\;\; \forall w, a \in \mathbb{R}^n
    \]
\end{definition}

Hay muchas elecciones posibles de función de activación, a continuación
se proponen algunas.

\subsubsection{Sigmoide}

La función sigmoide está definida como:
\begin{align*}
    \sigma \colon & \mathbb{R} \longrightarrow ]0,1[             \\
    \quad         & x \longmapsto \sigma(x) = \frac{1}{1+e^{-x}}
\end{align*}

Esta función es una de las más usada como función de activación. Entre sus principales ventajas está que es diferenciable, monótona y regular. Además, tiene dos características que serán usadas posteriormente:
\begin{equation} \label{sigmoid:1}
    \sigma(-x)=1-\sigma(x)
\end{equation}
\begin{equation} \label{sigmoid:2}
    \frac{d\sigma(x)}{dx}=\sigma(x)\sigma(-x)
\end{equation}

\begin{figure}[H]
    \includegraphics[width=7cm]{sigmoid.png}
    \centering
\end{figure}

\subsubsection{Tangente Hiperbólica}

La función tangente hiperbólica está definida como:
\begin{align*}
    f \colon & \mathbb{R} \longrightarrow ]0,1[                       \\
    \quad    & x \longmapsto f(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\end{align*}

En el gráfico de esta función, se puede apreciar que su derivada es más pronunciada que la de la función sigmoide. Esto permite a la red neuronal aprender de forma más rápida, haciendo el aprendizaje más eficiente.

\begin{figure}[H]
    \includegraphics[width=7cm]{tanh.png}
    \centering
\end{figure}

\subsubsection{Unidad Lineal Rectificada (ReLU)}

La función ReLU está definida por:
\begin{align*}
    f \colon & \mathbb{R} \longrightarrow ]0,1[ \
    \quad    & x \longmapsto f(x) = \max(0,x)
\end{align*}

Evidentemente, está función es mucho más eficiente, computacionalmente hablando, que las dos primeras alternativas. Sin embargo, presenta un gran problema: cuando los valores de entrada se aproximan o son inferiores a 0, la red no puede efectuar propagación hacia atrás, por lo que no puede aprender.

\begin{figure}[H]
    \includegraphics[width=7cm]{relu.png}
    \centering
\end{figure}


Por lo tanto, después de este razonamiento, el valor de cada neurona se calculará siguiendo la expresión:

\begin{equation}
    \label{eqn:1}
    a^{(n+1)}_j = f\left( b^{(n)}_j + \sum_{i=1}^n a^{(n)}_i w^{(n)}_i \right) \in [0,1]
\end{equation}

donde $b^{(n)}$ es otro valor arbitrario que va siendo actualizado para modificar el valor de salida de la
función de activación $f$.

\subsection{Notación}

Las redes neuronales suelen ser más complejas que la expuesta en la Figura \ref{redneuronal:1}. Normalmente,
se tienen muchos más inputs, pesos y capas escondidas. Por esa razón, una notación matricial para describir
las operaciones que se van a realizar es bastante adecuada. Como se verá en el siguiente capítulo, nuestra
red neuronal de interés solo tiene una capa escondida, así que se adaptará la notación a ese hecho:

\begin{itemize}
    \item Los subíndices $k$, $i$ y $j$ serán usados para denotar los valores de entrada, los valores de la
          capa escondido y los valores de salida, respectivamente.
    \item $x_1, \ldots x_K$ es el vector con los valores de entrada.
    \item $h_1, \ldots h_N$ es el vector con los valores de la capa escondida.
    \item $y_1, \ldots y_M$ es el vector con los valores de salida.
    \item $W = (w_{ki})$ es una matriz de dimensiones $N\times M$, donde $N$ es la dimensión
          del vector de entrada, $M$ la dimensión del vector de valores de la capa escondida y $w_{ki}$ es el peso de la conexión de la
          neurona $x_k$ con la neurona $h_i$.
    \item $W' = (w_{ij}')$ es una matriz de dimensiones $M\times Q$, donde $M$ es la dimensión
          del vector de valores de la capa escondida, $Q$ es la dimensión del vector de salida y $w_{ki}'$
          es el peso de la conexión de la neurona $h_i$ con la neurona $y_j$.
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            roundnode/.style={circle, draw=black, minimum size=3mm},
        ]

        \node[] at (0,5) {Entrada};
        \node[] at (3,5) {Capa escondida};
        \node[] at (6,5) {Salida};

        \node[roundnode]   at (0,0)   (inputK)  {$x_K$};
        \node[roundnode]   at (0,2)   (input3)  {$x_3$};
        \node[roundnode]   at (0,3)   (input2)  {$x_2$};
        \node[roundnode]   at (0,4)   (input1)  {$x_1$};

        \path (input3) -- (inputK) node [black, font=\Huge, midway, sloped] {$\dots$};

        \node[roundnode]   at (3,0.5)   (hiddenN)  {$h_N$};
        \node[roundnode]   at (3,2.5)   (hidden2)  {$h_2$};
        \node[roundnode]   at (3,3.5)   (hidden1)  {$h_1$};

        \path (hidden2) -- (hiddenN) node [black, font=\Huge, midway, sloped] {$\dots$};

        \node[roundnode]   at (6,0)   (outputM)  {$y_M$};
        \node[roundnode]   at (6,2)   (output3)  {$y_3$};
        \node[roundnode]   at (6,3)   (output2)  {$y_2$};
        \node[roundnode]   at (6,4)   (output1)  {$y_1$};

        \path (output3) -- (outputM) node [black, font=\Huge, midway, sloped] {$\dots$};

        \draw[-stealth] (input1) -- (hidden1);
        \draw[-stealth] (input2) -- (hidden1);
        \draw[-stealth] (input3) -- (hidden1);
        \draw[-stealth] (inputK) -- (hidden1);

        \draw[-stealth] (input1) -- (hidden2);
        \draw[-stealth] (input2) -- (hidden2);
        \draw[-stealth] (input3) -- (hidden2);
        \draw[-stealth] (inputK) -- (hidden2);

        \draw[-stealth] (input1) -- (hiddenN);
        \draw[-stealth] (input2) -- (hiddenN);
        \draw[-stealth] (input3) -- (hiddenN);
        \draw[-stealth] (inputK) -- (hiddenN);

        \draw[-stealth] (hidden1) -- (output1);
        \draw[-stealth] (hidden1) -- (output2);
        \draw[-stealth] (hidden1) -- (output3);
        \draw[-stealth] (hidden1) -- (outputM);

        \draw[-stealth] (hidden2) -- (output1);
        \draw[-stealth] (hidden2) -- (output2);
        \draw[-stealth] (hidden2) -- (output3);
        \draw[-stealth] (hidden2) -- (outputM);

        \draw[-stealth] (hiddenN) -- (output1);
        \draw[-stealth] (hiddenN) -- (output2);
        \draw[-stealth] (hiddenN) -- (output3);
        \draw[-stealth] (hiddenN) -- (outputM);


        \node[] at (1.5,0) {$W$};
        \node[] at (4.5,0) {$W'$};

    \end{tikzpicture}
    \caption{Red neuronal con una capa escondida}
    \label{redneuronal:2}
\end{figure}

Con esta nueva notación, la expresión \ref{eqn:1}, puede escribirse como:

\begin{equation}
    \label{eqn:2}
    h_i = f(u_i) = f\left(\sum_{k=1}^K w_{ki} x_k\right), \;\;\;\;\;
    y_j = f(u_i') = f\left(\sum_{i=1}^N w_{ij}' h_i\right)
\end{equation}

donde $u_i=\sum_{k=1}^K w_{ki} x_k$ y $u_i'=\sum_{i=1}^N w_{ij}' h_i$.

\section{Descenso de gradiente}

Una vez se han definido los elementos de una red neuronal, es necesario definir cómo esa red neuronal
va a \textit{aprender}. Para ello, primero es necesario encontrar una forma de medir cómo de correcto
es el resultado de nuestra red neuronal, es decir, se va a definir una función que mida el error obtenido.
Dado que los vectores de salida son numéricos, se puede tomar simplemente la suma de la diferencia al
cuadrado entre el resultado de la red $y = (y_1, \ldots, y_M)$ y la salida esperada $t = (t_1, \ldots, t_M)$:

\begin{equation}
    \label{eqn:3}
    E(x, t, W, W') = \frac{1}{2}\sum_{j=1}^M(y_j-t_j)^2
\end{equation}

A partir de las ecuaciones \ref{eqn:2}, la dependencia de $x$, $W$ y $W'$ en la definición del error se
ve de forma inmediata. El objetivo a lo largo de esta sección va a ser minimizar esa función multidimensional.
Para ello, es necesario primero definir el gradiente de una función.

\begin{definition}[Gradiente]
    Sea una función $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ parcialmente derivable en un punto $a\in\mathbb{R}^n$, se define como
    el gradiente de $f$ en $a$, al vector $\nabla f(a)\in\mathbb{R}^n$ dado por:
    \[
        \nabla f(a) = \left( \frac{\partial f}{\partial x_1} (a), \frac{\partial f}{\partial x_2} (a), \cdots, \frac{\partial f}{\partial x_n} (a)  \right)
    \]
    de modo que la $j$-ésima coordenada del vector gradiente de $f$ en $a$ es la derivada parcial con respecto a la $j$-ésima variable de $f$ en $a$, para todo $j\in\{1, \cdots, n\}$.
\end{definition}


El algoritmo que se va a usar se denomina \textit{descenso de gradiente} \cite{lemarechal2012cauchy}.
Su nombre proviene del hecho
que el gradiente de una función multidimensional da como resultado la dirección más \textit{empinada},
o también se puede decir que el gradiente, siempre va \textit{cuesta arriba}. Sea $f \colon \mathbb{R}^n \longrightarrow \mathbb{R}$
una función multidimensional y $\nabla f$ su gradiente, es trivial deducir que entonces $-\nabla f$
siempre va \textit{cuesta abajo}. Este hecho puede ser explotado para encontrar mínimos, en principio locales,
de $f$, usando la expresión:

\begin{equation}
    \label{eqn:gradient_descent}
    x_{t+1}=x_t-\epsilon\nabla f(x_t)
\end{equation}

donde $\epsilon$ es un escalar positivo, pequeño y arbitrario, denominado \textit{ratio de aprendizaje}.
El ratio de aprendizaje puede ser constante o adaptarse al número de iteraciones y al valor del gradiente en cada momento.

Ahora se puede introducir un problema importante al escoger la función de activación para la red neuronal, el \textit{vanishing gradient problem} \cite{hochreiter1991untersuchungen}.
Al usasr funciones de activación que tienen un gradiente pequeño, como es el caso de la tangente hiperbólica (cuyo gradiente está en el intervalo $(0,1]$), el error aprendido decrece exponencialmente
con el número de capas de la red. Esto se debe a que se usa la regla de la cadena para propagar el error, por lo que se multiplica $n$ veces ese gradiente, provocando que el error propagado que se obtiene
en la primera capa sea muy pequeño.

Hay otro problema complentario, denominado \textit{exploding gradient problem}, que ocurre justo cuando se usan funciones de activación con gradientes que toman valores más grandes \cite{philipp2017exploding}.

Ninguno de esos problemas afecta a la red neuronal de este trabajo, ya que solo tenemos 3 capas (es decir, solo computamos dos veces la derivada en el algoritmo de descenso de gradiente).

\subsection*{Convergencia}

A continuación se justifica porque la expresión anterior converge hacia un mínimo. Antes de continuar, es necesario definir algunos
conceptos nuevos:
\begin{definition}[Conjunto convexo]
    Un conjunto $K\subset \mathbb{R}^n$ se dice que es convexo si para cualesquiera $x,y\in K$, con $t\in[0,1]$, entonces $tx+(1-t)\in K$.
\end{definition}

\begin{definition}[Función convexa]\label{prop:convexa}
    Sea $K\subset \mathbb{R}^n$ un conjunto convexo y sea $f: K \rightarrow \mathbb{R}$. Se dice que $f$ es convexa si para cada $x,y\in K$ y cada $t \in [0,1]$, se da:
    \[
        f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)
    \]
\end{definition}

Las funciones convexas tienen una propiedad importante para nuestro objetivo:

\begin{proposition}\label{prop:convexo}
    Sea $K\subset \mathbb{R}^n$ convexo, y abierto y sea $f: K \rightarrow \mathbb{R}$ una función convexa. Entonces:
    \begin{enumerate}
        \item Cualquier mínimo local de $f$ es un mínimo global.
        \item Si $f$ es diferenciable, entonces para cualesquiera $x,y\in K$ se cumple:
        \begin{equation}\label{random:2}
            \nabla f(y)^T(x-y) \leq f(x) - f(y)
        \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}$ $
    \begin{enumerate}
        \item Sea $x^*$ un mínimo local. Eso implica que existe $r\in\mathbb{R}^+$ $\forall y \in B(x^*, r)$ tal que $f(x^*)\leq f(y)$. Sea $z\in K$, cumpliendo $f(z)<f(x^*)$, el objetivo
        es llegar a una contradicción.

        Como $K$ es convexo y $z\in K$, entonces el segmento $\overline{zx^*}$ está contenido en $K$. Sea $w\in \overline{zx^*}\cap B(x^*, r)$ con $w$ distinto de $z$ y $x^*$. Como $w$ es un punto
        del segmento $\overline{zx^*}$, existe $t^*\in ]0,1[$: $t^*x^*+(1-t^*)z=w$. Por tanto:
        \[
            f(w) = f(t^*x^*+(1-t^*)z)\leq t^*f(x^*)+(1-t^*)f(z) < t^*f(x^*) + (1-t^*)f(x^*) = f(x^*)
        \]
        \item Como $f$ es convexa, se tiene:
        \[
            f(y+t(x-y))-f(y) \leq t(f(x)-f(y))
        \]
        Ahora se puede simplificar por $t>0$ a ambos lados:
        \[
            \frac{f(y+t(x-y))-f(y)}{t}
        \]
        Tomando límite cuando $t \longrightarrow 0$, se obtiene:
        \[
            \lim_{t\longrightarrow 0 } \frac{f(y+t(x-y))-f(y)}{t} = \nabla f(y)^T)(x-y)=df_y(x-y) \leq f(x)-f(y)
        \]
    \end{enumerate}
\end{proof}

\begin{definition}[Gradiente lipschitziano]\label{def:gradiente_lips}
    Sea $K \subset \mathbb{R}^n$ y $f: K \rightarrow \mathbb{R}$ una función diferenciable. Sea dice que la aplicación gradiente de $f$ es \textit{lipschitziano} con constante $L\geq 0$ si:
    \[
        \norm{\nabla f(x) - \nabla f(y)} \leq L \norm{x-y} \;\; \forall x,y \in K
    \]
\end{definition}

\begin{proposition}
    Sea $K\subset \mathbb{R}^n$ convexo y $f: K \longrightarrow \mathbb{R}$ una función diferenciable. Si el gradiente de $f$ es lipschitziano con constante $L>0$, entonces:
    \begin{equation}\label{prop:ineq}
        f(y) \leq f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}\norm{y-x}^2, \;\;\; \forall x,y \in K
    \end{equation}
\end{proposition}

\begin{proof}
    Sea $x,y\in K$. Sea $g(t)=f(x+t(y-x))$ para cada $t\in[0,1]$. Entonces $g$ es derivable con:
    \[
        g'(t) = \nabla f(x+t(y-x))^T(y-x)
    \]
    para cada $t\in [0,1]$. Aplicando la desigualdad de Cauchy-Swartz y la definición de gradiente lipschitziano \ref{def:gradiente_lips}, se tiene:
    \begin{align*}
        g'(t) - g'(0) & = \left[ \nabla f(x+t(y-x))-\nabla f(x) \right]^T (y-x)\\
         & \leq \norm{\nabla f(x+t(y-x))-\nabla f(x)} \norm{y-x} \\
         & \leq Lt\norm{y-x}^2
    \end{align*}
    Y por último, aplicando el teorema fundamental del cálculo:
    \begin{align*}
        f(y) & = g(1) = g(0)  + \displaystyle\int_0^1g'(t)dt \leq g(0) + g'(0) + L\norm{y-x}^2\int_0^1tdt \\
             & = f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}\norm{y-x}^2
    \end{align*}
\end{proof}

Con esta última proposición, ya se disponen de todos los ingredientes necesarios para probar la convergencia del descenso de gradiente.

\begin{proposition}
    Sea $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ tal que $f\in C^1(\mathbb{R}^n)$, una función convexa con gradiente lipschitziano con constante
    $L>0$. Suponiendo que $x^*$ es un mínimo local de $f$. Sea $\eta \leq \frac{1}{L}$ y sea $x_0\in\mathbb{R}^n$. Se define
    \[
        x_k = x_{k-1} - \eta \nabla f(x_{k-1})
    \]
    para cada $k\in\mathbb{N}$. Entonces:
    \[
        f(x_k)-f(x^*) \leq \frac{\norm{x_0-x^*}^2}{2\eta k}
    \]
\end{proposition}

\begin{proof}
    Dado que $f$ es convexa, usando \ref*{prop:convexa}, es seguro que $x^*$ es un mínimo global. Sea $x\in\mathbb{R}^n$ un elemento fijo y sea
    $y=x-\eta\nabla (x)$. Usando la desigualdad \ref*{prop:ineq} se tiene que:
    \[
        f(y) \leq f(x) - \eta\norm{\nabla f(x)}^2 + \frac{L}{2}\eta^2\norm{\nabla f(x)}^2
    \]
    Como $\eta<\frac{1}{L}$:
    \begin{equation}\label{random:1}
        f(y) \leq f(x) - \frac{1}{2}\eta \norm{\nabla f(x)}^2
    \end{equation}
    En particular, $f(y)\leq f(x)$, y por tanto:
    \[
        f(x_{k+1}) \leq f(x_k)
    \]
    Lo cual demuestra que la actualización del gradiente descendente conlleva al decrecimiento de la función objetivo en cada iteración. Usando la desigualdad
    \ref*{random:2}, se obtiene:
    \[
        f(y) - f(x^*) \leq \frac{1}{2\eta}\left( 2\eta \nabla f(x)^T(x-x^*) - \eta^2\norm{f(x)}^2 \right)
    \]
    Sumando y restando $\norm{x-x^*}^2$ y usando:
    \[
        \norm{y-x^*}^2 = \norm{x-\eta \nabla f(x)-x^*}^2 = \norm{x-x^*}^2 + \eta^2\norm{\nabla f(x)}^2 - 2\eta \nabla f(x)^T(x-x^*),
    \]
    se llega a la siguiente desigualdad:
    \[
        f(y) - f(x^*) \leq \frac{1}{2\eta} \left( \norm{x-x^*}^2 - \norm{y-x^*}^2 \right)
    \]
    Para finalizar la demostración basta con aplicar la desigualdad anterior tomando $x=x_{i-1}$, $y=x_{i-1}-\eta\nabla f(x_{i-1})$ para cada
    $i$ de forma que:
    \begin{align*}
        k\left( f(x_k) - f(x^*) \right) & \leq \displaystyle \sum_{i=1}^k \left( f(x_i) - f(x^*) \right) \leq \displaystyle \sum_{i=1}^k \frac{1}{2\eta}\left( \norm{x_{i-1}-x^*} - \norm{x_i - x^*}^2 \right) \\
              & = \frac{1}{2\eta} \left( \norm{x_0 - x^*}^2 - \norm{x_k-x^*}^2 \right) \\
              & \leq \frac{1}{2\eta} \norm{x_0-x^*}^2
    \end{align*}
\end{proof}

\section{Back-propagation}

En el caso particular de redes neuronales, este algoritmo se usará para actualizar el valor de los pesos
de forma que el error definiendo en \ref{eqn:3}, sea mínimo. A este algoritmo, se le conoce como
\textit{back-propagation} \cite{rumelhart1986learning}. Adaptando la expresión del descenso de gradiente al caso más sencillo, se obtiene:

\begin{equation}
    \label{eqn:gradient_descent_nn}
    w^{(nueva)}=w^{(vieja)}-\epsilon (y-t)y(1-y)x
\end{equation}

donde $y$ es el valor de salida, $t$ el valor que se quiere y $x$ el valor de entrada. Para obtener esa
expresión en el caso de una red neuronal con una capa escondida, son necesarios algunos pasos adicionales.
Primero, es necesario derivar el error con respecto al valor de entrada:
\[
    \frac{\partial E}{\partial y_j} = y_j - t_j
\]
Una vez ese valor es conocido, se puede calcular la derivada respecto $u_j'$:
\[
    \frac{\partial E}{\partial u_j'} =     \frac{\partial E}{\partial y_j}     \frac{\partial y_j}{\partial u_j'}=
    (y_j-t_j)y_j(1-y_j) := EI_j'
\]
A este valor se le va a denotar $EI_j'$ porque es usado en los siguientes cálculos. Siguiendo ahora con $w_{ij}'$:
\[
    \frac{\partial E}{\partial w_{ij}'} =     \frac{\partial E}{\partial u_j'}     \frac{\partial u_j'}{\partial w_{ij}'} =
    EI_j' h_i
\]
Por lo tanto, aplicando el método de descenso de gradiente, se han obtenido las ecuaciones para actualizar los pesos
entre la capa escondida y la capa de salida:
\[
    w_{ij}'^{(nuevo)} = w_{ij}'^{(viejo)} - \epsilon \frac{\partial E}{\partial w_{ij}'} =
    w_{ij}'^{(viejo)}-\epsilon EI_j' h_i
\]

A continuación se va a realizar el paso que le da nombre a este algoritmo. Repitiendo el cálculo anterior
pero para la capa de entrada y la escondida, es decir, considerando $h_i$, $u_i$ y $w_{ki}$, es posible
actualizar su matriz de pesos teniendo en cuenta la optimización obtenida en la capa anterior. Por lo tanto,
derivando respecto de $h_i$:
\[
    \frac{\partial E}{\partial h_i} = \sum_{j=1}^M\frac{\partial E}{\partial u_j'} \frac{\partial u_j'}{\partial h_i} =
    \sum_{j=1}^M EI_j' w_{ij}'
\]
Continuando con $u_i$, una vez más se denota el valor obtenido por su importancia en cálculos posteriores:
\[
    \frac{\partial E}{\partial u_i} =     \frac{\partial E}{\partial h_i}     \frac{\partial h_i}{\partial u_i} =
    \sum_{j=1}^M  EU_j' w_{ij}' h_i (1-h_i) := EI_i
\]
Y por último:
\[
    \frac{\partial E}{\partial w_{ki}} = \frac{\partial E}{\partial u_i} \frac{\partial u_i}{\partial w_{ki}} =
    EI_i x_k
\]
Obteniendo, finalmente, la ecuación para actualizar los pesos entre la capa de entrada y la escondida:
\[
    w_{ki}^{(nuevo)} = w_{ki}^{(viejo)} - \epsilon \frac{\partial E}{\partial w_{ki}} =
    w_{ij}^{(viejo)}-\epsilon EI_i x_k
\]